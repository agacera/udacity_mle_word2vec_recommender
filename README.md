<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: notebooks/index.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

# Capstone Project: Word2Vec for Recommendation Systems

> Generating movie recommendations using word2vec


# Definition

## Project Overview

This project aims to show how [Word2vec](https://en.wikipedia.org/wiki/Word2vec) can be used to build recommendation systems.

A [recommendation system](https://en.wikipedia.org/wiki/Recommender_system) seeks to predict which items a given user would more likelly interact with (click, buy, listen, watch and etc) based on previous interactions made by him or by similar users. This is a big area of interest for research and industry due to its comproved efficacy to increase user engagement and per user revenue. As an example, Netflix estimates that more than 80% of shows people watch on their platform are discovered throught their recommendation systems ([source](https://www.wired.co.uk/article/how-do-netflixs-algorithms-work-machine-learning-helps-to-predict-what-viewers-will-like)).

Word2Vec is a set of technhiques to generate word embeddings for NLP tasks. Its main advantages over other NLP tecnhiques it that it learns the context of words in a sentence based on the surroundings of other words, an illustration of this ability is that the algorithm could understand when the word Buffalo is used to mean the animal buffalo or the city "Buffalo" in the state of New York in a sentence. It does this by representing the words in a multi-dimensional space (embeddings) and these embeddings have the property that the distance between the embeddings is a signal of its similarity.

The dataset used for this project is the [MovieLens](https://grouplens.org/datasets/movielens/), it is probably the most famous and used dataset for recommendation systems. Most libraries for recommendation systems and research articles use this dataset, so this simplifies the benchmarking.


The final project I implemented differed from my original proposal on the following points:

* Dataset

    I initially planed to develop this project against the ["Expedia Hotel Recommendations Kaggle competition"](https://www.kaggle.com/c/expedia-hotel-recommendations/data), however this dataset showed difficult to work due to its size, complexity of features and lack of articles. So after a time spent on EDA, I concluded that using a more common dataset for recommendations systems would be simpler to develop and to benchmark. For this reason, I decided to use the famous [MovieLens](https://grouplens.org/datasets/movielens/) dataset.

    Also, the main benefit in my opinion to use a Kaggle competion for this project was the possibility to easily benchmark my results against the leaderboard. But since my proposal is to demonstrate how a tecnhique of a different domain (NLP), can be used for recommendation systems, I dont expect that my final solution will beat purpose built algorithms.


* Algorithm

    My first idea was to build a custom model in PyTorch to generate the embeddings. However, I ended up using [Gensim](https://radimrehurek.com/gensim/) a common library for word2vec. This approach allowed me to focus more on the technique and in other parts of my project.



## Problem Statement

This problem is defined as: Applying Word2Vec to learn similar movies using the MovieLens dataset.

This dataset is derived from the [MovieLens platform](https://movielens.org/), there users can rate and review movies they have watched, get rich information about movies and receive recommendations of movies to watch next.

In this project I used the smaller version of the dataset called MovieLens100K which contains 100000 of ratings of 9000 movies, made by 600 users ([link for dataset](https://grouplens.org/datasets/movielens/latest/)).

The goal here is to demonstrate how a tecnhique of NLP can be abbused to generate recommendations, I explore ways to serve the recommendations based on embeddings in a scalable way and I also create a sample application to visualize the generated recommendations.

I dont intend to produce state of the art results, but simply demonstrate a good enough and simple tecnhique to generate recommendations.

## Metrics

The main metric I used to evaluate my model was "Average Precision at k" ([source](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision)). This is a common metric used by recommendation systems and also simple to calculate.

Since the recommendation system I built suggests similar items (item-to-item) and the MovieLens dataset doesn't have a groundtruth dataset for item similarity, I calculated the metric using the following approach: 

1. Isolated X% of my users to use for evaluation
1. For each user U I selected all his positive movie ratings MR
1. For each MR I calculated 5 suggestions for the movie rated
1. I checked if the suggested movies were inside the MR, if so, I considered it as a match. I calculated the precision P for each recommendation.
1. I sumed all P and divided by the number of users to have my final average precision at 5.

The source code for the way I calculated this metric can be found [here](TODO).

# Analysis

## Data Exploration

The data exploration I performed for this project can be found on the notebook [10_eda](./10_eda.ipynb).

The dataset used is the MovieLens 100k. It contains the following files:
<div class="codecell" markdown="1">
<div class="input_area" markdown="1">

```
!ls -hl ../data/ml-latest-small
```

</div>
<div class="output_area" markdown="1">

    total 6472
    -rw-r--r--  1 felipe.gasparini  2119619082   8.1K Sep 26  2018 README.txt
    -rw-r--r--  1 felipe.gasparini  2119619082   193K Sep 26  2018 links.csv
    -rw-r--r--  1 felipe.gasparini  2119619082   483K Sep 26  2018 movies.csv
    -rw-r--r--  1 felipe.gasparini  2119619082   2.4M Sep 26  2018 ratings.csv
    -rw-r--r--  1 felipe.gasparini  2119619082   116K Sep 26  2018 tags.csv


</div>

</div>

In this project I focused on the `ratings.csv` file since it had the the user interactions, which is what I needed to train my model.
<div class="codecell" markdown="1">
<div class="input_area" markdown="1">

```
import pandas as pd
```

</div>

</div>
<div class="codecell" markdown="1">
<div class="input_area" markdown="1">

```
ratings_df = pd.read_csv("../data/ml-latest-small/ratings.csv")
ratings_df.head()
```

</div>
<div class="output_area" markdown="1">




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>userId</th>
      <th>movieId</th>
      <th>rating</th>
      <th>timestamp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>4.0</td>
      <td>964982703</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>3</td>
      <td>4.0</td>
      <td>964981247</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>6</td>
      <td>4.0</td>
      <td>964982224</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>47</td>
      <td>5.0</td>
      <td>964983815</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>50</td>
      <td>5.0</td>
      <td>964982931</td>
    </tr>
  </tbody>
</table>
</div>



</div>

</div>
<div class="codecell" markdown="1">
<div class="input_area" markdown="1">

```
ratings_df.shape
```

</div>
<div class="output_area" markdown="1">




    (100836, 4)



</div>

</div>

The ratings dataset is quite simple. It consists of the userId, movieId, rating and timestamp.

There are over 100000 ratings in the dataset and they are not evenly distributed across the dimensions: users, movies and ratings, as we can observe:
<div class="codecell" markdown="1">
<div class="input_area" markdown="1">

```
ratings_df["userId"].value_counts().plot(kind='hist', bins=30, title="Users frequency")
```

</div>
<div class="output_area" markdown="1">




    <matplotlib.axes._subplots.AxesSubplot at 0x7ffa20a4c3d0>




![png](output_12_1.png)


</div>

</div>
<div class="codecell" markdown="1">
<div class="input_area" markdown="1">

```
ratings_df["movieId"].value_counts().plot(kind='hist', bins=30, title="Movies frequency")
```

</div>
<div class="output_area" markdown="1">




    <matplotlib.axes._subplots.AxesSubplot at 0x7ffa20971590>




![png](output_13_1.png)


</div>

</div>

We can observe a few properties that are common in recommendation systems:

1. few users generates most of the interactions
1. few items concetrate most of the interactions

The second property is referenced as long-tail, and it is common for recommender systems to learn to recommend just the most popular items (this is know as the richer-get-richer problem) and perform well during evaluation and in production, but depending of the business, it may be interesting to diversify more the recommendations. The property of a recommender system to provide diverse recommendations is called serendipity and there is a huge collection of research articles about this ([source](https://scholar.google.de/scholar?q=recommender+systems+serendipity&hl=en&as_sdt=0&as_vis=1&oi=scholart)).

To illustrate this property in this domain (movies), consider the movie "The Lord of the Rings - A fellowship of the Ring", users that saw this movie intuitivelly are more likelly to also watch the other movies of the "Lord of the Rings" franchise and "The Hobbit", so these recommendations are obvious.

I'm not planning to tackle the serendipity problem here.
<div class="codecell" markdown="1">
<div class="input_area" markdown="1">

```
ratings_df["rating"].plot(kind='hist', title="Ratings frequency")
```

</div>
<div class="output_area" markdown="1">




    <matplotlib.axes._subplots.AxesSubplot at 0x7ffa20756e90>




![png](output_15_1.png)


</div>

</div>
<div class="codecell" markdown="1">
<div class="input_area" markdown="1">

```
ratings_df_with_positive = ratings_df
ratings_df_with_positive["is_positive_review"] = ratings_df_with_positive["rating"] >= 3.0
ratings_df_with_positive["is_positive_review"].value_counts()
```

</div>
<div class="output_area" markdown="1">




    True     81763
    False    19073
    Name: is_positive_review, dtype: int64



</div>

</div>

I used the threshold value of 3 to consider a review as positive. Since I only use positive reviews in my model, this removed almost 20000 ratings from the dataset.

### Algorithms and Techniques

This project is unusual since I dont try to find the best machine learning model and tune it against a metric. Here I try to apply to recommendation systems a tecnhnique from Natural Language Processing.

There are two techniques I'm using here:

1. Generate item embeddings using Word2Vec
1. Find similar items (generate recommendations) with embeddings

#### Generate item embeddings

There are multiple ways to generate [embeddings](https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526), word2vec is just one approach to it.

Word2vec works by learning the context of words in a sentence. So the problem I had to solve was to how to generate a sentence to use for training.

A usual sentence for word2vec is something like: 'A rabbit is in a hole', so a sentence is a real language sentence. The first approach I used was to form sentences for each review using the movieId and the genres of the movie, so a sentence was "8217312 action drama", this didnt work well: instead of learning the similarities between movies (movieIds) it actually learned the similarities across categories. Which was not what I intended and not very useful.

After a few more interations I generated sentences using the following algorithm:

1. sort all interactions by userId and timestamp. So all interactions of a user are together and they are sorted by timestamp
1. create a sentence consisting of all movies a user has rated

With this approach, a sample sentence has this format: "318391 2312 123923 763 6123".

The assumption I made was that users watch and review similar movies close to each other, so the word2vec model should learn how frequent these movies are watched together and create embeddings that represent these similarities.

#### Find similar items using embeddings

The output of the model training are embeddings. To generate recommendations for a movie, I have to get its representation as embedding, search for the K closest embeddings and for each of the embeddings found, discover the movieId for this movie.

The problem of finding the K closest embeddings is just classic problem to find the [K Nearest Neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). 

For this project, I implemented a recommender ([source](./02_recommender.ipynb)) completely decoupled from the Gensim library (the library used to generate the embeddings). This approach allows the separation between training and predicting. This can simplify the deployment since you dont need to include in your runtime the libraries used for training. Since the recommender is decoupled from Gensim, it could also be used to servce recommendations created by other models that also generate embeddings.

Also, KNN scales linearly with the cardinality of the embeddings, so for problems with milions or billions of embeddings, a naive approach of KNN would not work - it would take too long to produce any recommendation. This is an interesting topic and industry are tackling this problem with creative ways: Facebook created a library for performatic aproximate KNN search called FAISS (https://github.com/facebookresearch/faiss) and other companies, are storing the embeddings in search engines like Solr to levarege its distributed nature to speed up queries.


### Benchmark

The MovieLens dataset is widely used for recommendation systems, so benchmarks are widely available. Microsoft maintains this [repository](https://github.com/microsoft/recommenders) with several benchmarks of recommendation systems applied in the MovieLens dataset. 

My initial idea is that I could simply compare the results I got against this [benchmark](https://github.com/microsoft/recommenders/blob/master/benchmarks/movielens.ipynb), however, it turned out more difficult than expected due to how I calculate my metrics or the need to standartize my model training using this benchmark parameters.

Due to time constraints I didnt have time to benchmark the performance of my recommendation system. However, since my goal is to demonstrate the tecnhique, I was not considering the benchmark as a must have in this project.

Also, my professional experience, shows that offline metrics hardly correlates to online metrics for recommendations systems. So I deployed one web application to show the results of the recommender I built and anyone can visualize the recommendations generated. This was also useful to demonstrate how to deploy this model in production.

You can check the we-application on here: TODO

## Methodology

### Data Preprocessing



## Appendix - References

* https://www.analyticsvidhya.com/blog/2019/07/how-to-build-recommendation-system-word2vec-python/
